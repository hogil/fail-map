#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Dual Bucket Pipeline - Memory Optimized Version
ë©”ëª¨ë¦¬ ì ˆì•½ ìš°ì„ : ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹, ì‘ì€ ë©”ëª¨ë¦¬ í’‹í”„ë¦°íŠ¸
- ì²­í¬ë³„ ìˆœì°¨ ì²˜ë¦¬ (í•œ ë²ˆì— í•˜ë‚˜ì”©)
- ì‘ì€ ThreadPool (64 threads)
- í° chunk sizeë¡œ ì ì€ ë°˜ë³µ
- ì¦‰ì‹œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
- íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
"""
import os, re, sys, time, json, io, zipfile, tempfile, shutil, gzip, tarfile, subprocess, importlib, multiprocessing
from dataclasses import dataclass
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from pathlib import Path
from collections import defaultdict, Counter
import gc

import boto3
import pandas as pd
import numpy as np
from PIL import Image, ImageDraw, ImageFont
from tqdm import tqdm
from botocore.config import Config

# =================== Memory-First Config ===================

@dataclass
class MemoryFirstConfig:
    bucket_name: str = 'eds-ec-memory.fbm-data'
    region_name: str = ''
    aws_access_key_id: str = 'ho.choi-LakeS3-F6B0U6'
    aws_secret_access_key: str = 'iYb7zYDVzitt4QVkUcR2'
    endpoint_url: str = 'http://lakes3.dataplatform.samsungds.net:9020'

    # ë©”ëª¨ë¦¬ ìµœì í™”: ì ì€ ì—°ê²°ê³¼ ìŠ¤ë ˆë“œ
    max_pool_connections: int = 128
    download_threads: int = 64  # ê¸°ë³¸ 128 â†’ 64
    cpu_processes: int = min(multiprocessing.cpu_count() // 2, 12)  # ì ˆë°˜ë§Œ ì‚¬ìš©
    chunk_size: int = 600  # ë” í° ì²­í¬ (300 â†’ 600)

    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
    aggressive_gc: bool = True
    gc_interval: int = 1  # ë§¤ ì²­í¬ë§ˆë‹¤

    border_thickness: int = 1
    defect_border_thickness: int = 2
    default_tile_size: tuple = (24, 24)
    draw_empty_chip_text: bool = True
    empty_chip_text_field: str = "b"
    palette_colors: int = 32
    color_json: str = "/appdata/appuser/l3tracker-main/logs/color-legends.json"
    folder_filter_middle: str = "-00P_"
    hours_back_start: int = 0
    hours_back_end: int = 2
    df_path: str = "/appdata/appuser/project/device_info.txt"
    df_positions: tuple = (4, 3, 1)
    base_root: str = "/appdata/appuser/images"
    positions_root: str = "/appdata/appuser/positions"

@dataclass
class BucketBMemoryConfig:
    bucket_name: str = 'eds-ec-memory.fbm-data-secondary'
    region_name: str = ''
    aws_access_key_id: str = 'ho.choi-LakeS3-F6B0U6'
    aws_secret_access_key: str = 'iYb7zYDVzitt4QVkUcR2'
    endpoint_url: str = 'http://lakes3.dataplatform.samsungds.net:9020'
    max_pool_connections: int = 128
    download_threads: int = 64
    enabled: bool = True
    time_offset_range: tuple = (0, 10)

    # ë©”ëª¨ë¦¬ ìµœì í™”: ìˆœì°¨ ì²˜ë¦¬
    sequential_download: bool = True  # Bucket A ì´í›„ Bucket B ë‹¤ìš´ë¡œë“œ
    streaming_mode: bool = True  # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹

CFG = MemoryFirstConfig()
CFG_B = BucketBMemoryConfig()

print(f"ğŸ’¾ MEMORY-EFFICIENT MODE: threads={CFG.download_threads}, processes={CFG.cpu_processes}, chunk_size={CFG.chunk_size}")

class StreamingProcessor:
    """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì†Œí™”"""

    @staticmethod
    def process_sequential(contents_a, contents_b):
        """ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½"""
        # A ì²˜ë¦¬
        for item in contents_a:
            yield ('a', item)
            del item  # ì¦‰ì‹œ ì‚­ì œ

        # A ì™„ë£Œ í›„ B ì²˜ë¦¬
        for item in contents_b:
            yield ('b', item)
            del item

    @staticmethod
    def cleanup():
        """ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜"""
        gc.collect()

if __name__ == "__main__":
    print("ğŸ’¾ Memory-Efficient Dual Bucket Pipeline")
    print("   - ì ì€ ìŠ¤ë ˆë“œ (64)")
    print("   - ìˆœì°¨ ì²˜ë¦¬")
    print("   - í° ì²­í¬ (600)")
    print("   - ê°•ì œ GC")
    print("   - ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ")
